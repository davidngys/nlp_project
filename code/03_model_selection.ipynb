{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSI19 Project 3 - Model Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Establishing Baseline](#chapter1)\n",
    "* [2. Data Preparation](#chapter2)\n",
    "    * [2.1 Create Train/Test Data](#chapter2_1)\n",
    "    * [2.2 Create X and y Variables](#chapter2_2)\n",
    "* [3. Model Selection](#chapter3)\n",
    "    * [3.1 Creating Pipelines](#chapter3_1)\n",
    "    * [3.2 Defining Function](#chapter3_2)\n",
    "    * [3.3 Gathering Results](#chapter3_3)\n",
    "    * [3.4 Results Analysis and Model Selection](#chapter3_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Establishing Baseline <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the problem is a classification problem, the baseline score will be the majority of the 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('../data/processed_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_raw</th>\n",
       "      <th>is_tifu</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_base</th>\n",
       "      <th>text_lem</th>\n",
       "      <th>text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TIFU By accidentally being racist to an Asian ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['tifu', 'by', 'accidentally', 'being', 'racis...</td>\n",
       "      <td>tifu by accidentally being racist to an asian ...</td>\n",
       "      <td>tifu by accidentally being racist to an asian ...</td>\n",
       "      <td>tifu by accident be racist to an asian friend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TIFU by ordering dish towels from Amazon (NSFW...</td>\n",
       "      <td>1</td>\n",
       "      <td>['tifu', 'by', 'ordering', 'dish', 'towels', '...</td>\n",
       "      <td>tifu by ordering dish towels from amazon nsfw ...</td>\n",
       "      <td>tifu by ordering dish towel from amazon nsfw o...</td>\n",
       "      <td>tifu by order dish towel from amazon nsfw obli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIFU because I told the teacher I like drugs P...</td>\n",
       "      <td>1</td>\n",
       "      <td>['tifu', 'because', 'told', 'the', 'teacher', ...</td>\n",
       "      <td>tifu because told the teacher like drugs prett...</td>\n",
       "      <td>tifu because told the teacher like drug pretty...</td>\n",
       "      <td>tifu becaus told the teacher like drug pretti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TIFU by losing my phone and getting picked up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['tifu', 'by', 'losing', 'my', 'phone', 'and',...</td>\n",
       "      <td>tifu by losing my phone and getting picked up ...</td>\n",
       "      <td>tifu by losing my phone and getting picked up ...</td>\n",
       "      <td>tifu by lose my phone and get pick up by campu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TIFU by setting out a digital picture frame TI...</td>\n",
       "      <td>1</td>\n",
       "      <td>['tifu', 'by', 'setting', 'out', 'digital', 'p...</td>\n",
       "      <td>tifu by setting out digital picture frame tifu...</td>\n",
       "      <td>tifu by setting out digital picture frame tifu...</td>\n",
       "      <td>tifu by set out digit pictur frame tifu by set...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text_raw  is_tifu  \\\n",
       "0  TIFU By accidentally being racist to an Asian ...        1   \n",
       "1  TIFU by ordering dish towels from Amazon (NSFW...        1   \n",
       "2  TIFU because I told the teacher I like drugs P...        1   \n",
       "3  TIFU by losing my phone and getting picked up ...        1   \n",
       "4  TIFU by setting out a digital picture frame TI...        1   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  ['tifu', 'by', 'accidentally', 'being', 'racis...   \n",
       "1  ['tifu', 'by', 'ordering', 'dish', 'towels', '...   \n",
       "2  ['tifu', 'because', 'told', 'the', 'teacher', ...   \n",
       "3  ['tifu', 'by', 'losing', 'my', 'phone', 'and',...   \n",
       "4  ['tifu', 'by', 'setting', 'out', 'digital', 'p...   \n",
       "\n",
       "                                           text_base  \\\n",
       "0  tifu by accidentally being racist to an asian ...   \n",
       "1  tifu by ordering dish towels from amazon nsfw ...   \n",
       "2  tifu because told the teacher like drugs prett...   \n",
       "3  tifu by losing my phone and getting picked up ...   \n",
       "4  tifu by setting out digital picture frame tifu...   \n",
       "\n",
       "                                            text_lem  \\\n",
       "0  tifu by accidentally being racist to an asian ...   \n",
       "1  tifu by ordering dish towel from amazon nsfw o...   \n",
       "2  tifu because told the teacher like drug pretty...   \n",
       "3  tifu by losing my phone and getting picked up ...   \n",
       "4  tifu by setting out digital picture frame tifu...   \n",
       "\n",
       "                                           text_stem  \n",
       "0  tifu by accident be racist to an asian friend ...  \n",
       "1  tifu by order dish towel from amazon nsfw obli...  \n",
       "2  tifu becaus told the teacher like drug pretti ...  \n",
       "3  tifu by lose my phone and get pick up by campu...  \n",
       "4  tifu by set out digit pictur frame tifu by set...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(text_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.502014\n",
      "0    0.497986\n",
      "Name: is_tifu, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "print(text_df['is_tifu'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline score if 50.2% for the `/r/tifu` subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of model selection, our feature will be `text_base` and target will be `is_tifu`.\n",
    "\n",
    "20% of the data will be stripped out as the test set that will be used for model evaluation once the best model has been selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Train/Test Data <a class=\"anchor\" id=\"chapter2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a train and test set\n",
    "test_csv = text_df.sample(frac=.2)\n",
    "train_csv = text_df.drop(list(test_csv.index),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_raw</th>\n",
       "      <th>is_tifu</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_base</th>\n",
       "      <th>text_lem</th>\n",
       "      <th>text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text_raw, is_tifu, text_token, text_base, text_lem, text_stem]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that test data extracted is no longer in train data\n",
    "display(test_csv[test_csv['text_raw'].isin(train_csv['text_raw']).astype(int) ==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test data as csv files\n",
    "train_csv.to_csv('../data/train.csv',index=False)\n",
    "test_csv.to_csv('../data/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create `X` and `y` Variables <a class=\"anchor\" id=\"chapter2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `train.csv`, the data will be split into a training data set and a validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y variables\n",
    "X = train_csv['text_base']\n",
    "y = train_csv['is_tifu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the data has been prepared into a `training` and `validation` set, the model pipeline will now be defined in order to cross validate and select the best performing model.\n",
    "\n",
    "Pipelines will be created based on the following.\n",
    "\n",
    "Feature Selection Tools:\n",
    "- CountVectorizer\n",
    "- TfidVectorizer\n",
    "\n",
    "Classification model\n",
    "- MultinomialBayes\n",
    "- LogisticRegression\n",
    "- KNearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating Pipelines <a class=\"anchor\" id=\"chapter3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary for 2 text feature extraction tools, with English stop words\n",
    "text_feature = {'cvec' : CountVectorizer(stop_words='english'), \n",
    "                'tvec': TfidfVectorizer(stop_words='english')}\n",
    "\n",
    "# Creading dictionary for classification models\n",
    "model = {'multi_nb':MultinomialNB(),\n",
    "        'knn3':KNeighborsClassifier(n_neighbors=3,n_jobs=-1),\n",
    "        'knn5':KNeighborsClassifier(n_neighbors=5,n_jobs=-1),\n",
    "        'lr':LogisticRegression(max_iter=2000,n_jobs=-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(steps=[('cvec', CountVectorizer(stop_words='english')),\n",
       "                 ('multi_nb', MultinomialNB())]),\n",
       " Pipeline(steps=[('cvec', CountVectorizer(stop_words='english')),\n",
       "                 ('knn3', KNeighborsClassifier(n_jobs=-1, n_neighbors=3))]),\n",
       " Pipeline(steps=[('cvec', CountVectorizer(stop_words='english')),\n",
       "                 ('knn5', KNeighborsClassifier(n_jobs=-1))]),\n",
       " Pipeline(steps=[('cvec', CountVectorizer(stop_words='english')),\n",
       "                 ('lr', LogisticRegression(max_iter=2000, n_jobs=-1))]),\n",
       " Pipeline(steps=[('tvec', TfidfVectorizer(stop_words='english')),\n",
       "                 ('multi_nb', MultinomialNB())]),\n",
       " Pipeline(steps=[('tvec', TfidfVectorizer(stop_words='english')),\n",
       "                 ('knn3', KNeighborsClassifier(n_jobs=-1, n_neighbors=3))]),\n",
       " Pipeline(steps=[('tvec', TfidfVectorizer(stop_words='english')),\n",
       "                 ('knn5', KNeighborsClassifier(n_jobs=-1))]),\n",
       " Pipeline(steps=[('tvec', TfidfVectorizer(stop_words='english')),\n",
       "                 ('lr', LogisticRegression(max_iter=2000, n_jobs=-1))])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in range(len(text_feature)*len(model)): # Look through number of possible combinations for pipelines\n",
    "    \n",
    "    pipelines = [] # Create list of empty pipelines\n",
    "    \n",
    "    for text, selector in text_feature.items(): # Iterate through text feature extraction dictionary\n",
    "        for name, mode in model.items(): # Iterate through classification model dictionary\n",
    "            pipe = Pipeline([            # Create pipeline\n",
    "                (text,selector),\n",
    "                (name,mode)\n",
    "            ])\n",
    "            pipelines.append(pipe) # Append pipeline to list\n",
    "\n",
    "display(pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Defining Function <a class=\"anchor\" id=\"chapter3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(X,y): # Takes in X, and y data\n",
    "    \n",
    "    # Train test split to create train and validation data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y) \n",
    "    \n",
    "    # Parameter grid for CountVectorizer\n",
    "    cvec_params = {\n",
    "        'cvec__max_features': [50, 100, 150],\n",
    "        'cvec__min_df': [1, 2, 3],\n",
    "        'cvec__max_df': [.5, .6, .9],\n",
    "        'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    }\n",
    "    \n",
    "    # Parameter grid for TfidVectorizer\n",
    "    tvec_params = {\n",
    "        'tvec__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "        'tvec__min_df' : [1, 2, 3],\n",
    "        'tvec__max_df' : [.5,.6,.8, .9]\n",
    "    }\n",
    "    \n",
    "    # Creating lists to store outputs for each pipe\n",
    "    cross_score = [] # Cross validation score\n",
    "    opt_params = [] # Optimal parameters from gridsearch\n",
    "    train_score = [] # Score on training set\n",
    "    val_score = [] # Score on validation set\n",
    "    tn_list = [] # True negatives predicted, interpreted as predicted /r/confessions and is /r/confessions\n",
    "    fp_list = [] # False positives predicted, interpreted as predicted /r/tifu and is /r/confessions\n",
    "    fn_list = [] # False negatives predicted, interpreted as predicted /r/confessions and is /r/tifu\n",
    "    tp_list = [] # True positives predicted, interpreted as predicted /r/tifu and is /r/tifu\n",
    "    \n",
    "    for pipe in pipelines: # Iteration through all the pipes created earlier\n",
    "        \n",
    "        cross = cross_val_score(pipe,X_train,y_train,cv=5).mean() # Obtaining cross validation score of the pipe\n",
    "        cross_score.append(cross) # Appending cross validation score to list\n",
    "        \n",
    "        if pipe.steps[0][0] == 'cvec': # Checking which text feature extraction tool is used in the pipe\n",
    "            params = cvec_params # Uses cvec param grid if cvec\n",
    "        else:\n",
    "            params = tvec_params # Uses tvec param grid if tvec\n",
    "        \n",
    "        # Grid search for optimal params\n",
    "        gs = GridSearchCV(pipe,\n",
    "                          param_grid=params,\n",
    "                          cv=5,\n",
    "                          verbose=1)\n",
    "        \n",
    "        # Fit Grid search on training data\n",
    "        gs.fit(X_train,y_train)\n",
    "        \n",
    "        # Obtaining outputs of confusion matrix, based on fitted grid search\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, gs.predict(X_val)).ravel()\n",
    "        \n",
    "        opt_params.append(gs.best_params_) # Appending best parameters of pipeline\n",
    "        train_score.append(gs.best_score_) # Appending best score based on training data\n",
    "        val_score.append(gs.score(X_val,y_val)) # Appending score bast on validation data\n",
    "        tn_list.append(tn) # Appending number of true negatives\n",
    "        fp_list.append(fp) # Appending number of false positives\n",
    "        fn_list.append(fn) # Appending number of false negatives\n",
    "        tp_list.append(tp) # Appending number of true positives\n",
    "        \n",
    "        print(gs.best_score_)\n",
    "        print(gs.best_params_)    \n",
    "        \n",
    "    # Creating dataframe to store outputs\n",
    "    eval_df = pd.DataFrame([cross_score,\n",
    "                            opt_params,\n",
    "                            train_score,\n",
    "                            val_score,\n",
    "                            tn_list,\n",
    "                            fp_list,\n",
    "                            fn_list,\n",
    "                            tp_list],\n",
    "                          index=['crossval_score','opt_params','train_score','val_score','tn','fp','fn','tp']).T\n",
    "    \n",
    "    return eval_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Gathering Results <a class=\"anchor\" id=\"chapter3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function will gather the following for all the pipelines created earlier:\n",
    "- `crossval_score`: Cross validation score\n",
    "- `opt_params`: Optimal parameters for text feature extraction tool, either `CountVectorizer` or `TfidVectorizer`\n",
    "- `train_score`: Best accuracy score on train data set\n",
    "- `val_score`: Best accuracy score on validation data set\n",
    "- `tn`: Number of True Negatives\n",
    "- `fp`: Number of False Positives\n",
    "- `fn`: Number of False Negatives\n",
    "- `tp`: Number of True Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 405 out of 405 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8900284800112512\n",
      "{'cvec__max_df': 0.6, 'cvec__max_features': 150, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 405 out of 405 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740620934566294\n",
      "{'cvec__max_df': 0.6, 'cvec__max_features': 50, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 3)}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 405 out of 405 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8790935621110367\n",
      "{'cvec__max_df': 0.6, 'cvec__max_features': 50, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 3)}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 405 out of 405 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9941281952111389\n",
      "{'cvec__max_df': 0.6, 'cvec__max_features': 50, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614675995921381\n",
      "{'tvec__max_df': 0.6, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7682676417847474\n",
      "{'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec__ngram_range': (1, 2)}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7808551035476952\n",
      "{'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec__ngram_range': (1, 1)}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9311803382440843\n",
      "{'tvec__max_df': 0.8, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# Run function on X and y data\n",
    "eval_df = model_eval(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crossval_score</th>\n",
       "      <th>opt_params</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858943</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 15...</td>\n",
       "      <td>0.890028</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>146</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.506297</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 50...</td>\n",
       "      <td>0.874062</td>\n",
       "      <td>0.889447</td>\n",
       "      <td>183</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501262</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 50...</td>\n",
       "      <td>0.879094</td>\n",
       "      <td>0.899497</td>\n",
       "      <td>186</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.979853</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 50...</td>\n",
       "      <td>0.994128</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.832945</td>\n",
       "      <td>{'tvec__max_df': 0.6, 'tvec__min_df': 3, 'tvec...</td>\n",
       "      <td>0.861468</td>\n",
       "      <td>0.819095</td>\n",
       "      <td>140</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.766598</td>\n",
       "      <td>{'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...</td>\n",
       "      <td>0.768268</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>137</td>\n",
       "      <td>61</td>\n",
       "      <td>43</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.780855</td>\n",
       "      <td>{'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...</td>\n",
       "      <td>0.780855</td>\n",
       "      <td>0.746231</td>\n",
       "      <td>148</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.910197</td>\n",
       "      <td>{'tvec__max_df': 0.8, 'tvec__min_df': 3, 'tvec...</td>\n",
       "      <td>0.93118</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>177</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  crossval_score                                         opt_params  \\\n",
       "0       0.858943  {'cvec__max_df': 0.6, 'cvec__max_features': 15...   \n",
       "1       0.506297  {'cvec__max_df': 0.6, 'cvec__max_features': 50...   \n",
       "2       0.501262  {'cvec__max_df': 0.6, 'cvec__max_features': 50...   \n",
       "3       0.979853  {'cvec__max_df': 0.6, 'cvec__max_features': 50...   \n",
       "4       0.832945  {'tvec__max_df': 0.6, 'tvec__min_df': 3, 'tvec...   \n",
       "5       0.766598  {'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...   \n",
       "6       0.780855  {'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...   \n",
       "7       0.910197  {'tvec__max_df': 0.8, 'tvec__min_df': 3, 'tvec...   \n",
       "\n",
       "  train_score val_score   tn  fp  fn   tp  \n",
       "0    0.890028  0.854271  146  52   6  194  \n",
       "1    0.874062  0.889447  183  15  29  171  \n",
       "2    0.879094  0.899497  186  12  28  172  \n",
       "3    0.994128  0.994975  197   1   1  199  \n",
       "4    0.861468  0.819095  140  58  14  186  \n",
       "5    0.768268  0.738693  137  61  43  157  \n",
       "6    0.780855  0.746231  148  50  51  149  \n",
       "7     0.93118  0.919598  177  21  11  189  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the gathered results, a few features will be created to evaluate the models\n",
    "- label the respective pipelines\n",
    "- compute sensitivity score\n",
    "- compute specificity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuples for labeling the pipelines\n",
    "steps = []\n",
    "for pipe in pipelines:\n",
    "    (x, y) = ( pipe.steps[0][0], pipe.steps[1][0] )\n",
    "    steps.append((x,y))\n",
    "    \n",
    "eval_df['steps'] = steps # Add column of labels to evaluation dataframe\n",
    "\n",
    "# Compute sensitivity score\n",
    "eval_df['sensitivity'] = eval_df['tp'] / (eval_df['tp']+eval_df['fn'])\n",
    "\n",
    "# Compute specificity score\n",
    "eval_df['specificity'] = eval_df['tn'] / (eval_df['tn']+eval_df['fp'])\n",
    "\n",
    "# Re-order columns for visualisation\n",
    "eval_df = eval_df[['steps', \n",
    "                   'crossval_score',\n",
    "                   'opt_params', \n",
    "                   'train_score', \n",
    "                   'val_score', \n",
    "                   'tn', \n",
    "                   'fp', \n",
    "                   'fn', \n",
    "                   'tp', \n",
    "                   'sensitivity', \n",
    "                   'specificity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Results Analysis and Model Selection <a class=\"anchor\" id=\"chapter3_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>crossval_score</th>\n",
       "      <th>opt_params</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(cvec, multi_nb)</td>\n",
       "      <td>0.858943</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 15...</td>\n",
       "      <td>0.890028</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>146</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>194</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.737374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(cvec, knn3)</td>\n",
       "      <td>0.506297</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 50...</td>\n",
       "      <td>0.874062</td>\n",
       "      <td>0.889447</td>\n",
       "      <td>183</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>171</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.924242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(cvec, knn5)</td>\n",
       "      <td>0.501262</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 50...</td>\n",
       "      <td>0.879094</td>\n",
       "      <td>0.899497</td>\n",
       "      <td>186</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>172</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(cvec, lr)</td>\n",
       "      <td>0.979853</td>\n",
       "      <td>{'cvec__max_df': 0.6, 'cvec__max_features': 50...</td>\n",
       "      <td>0.994128</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.994949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(tvec, multi_nb)</td>\n",
       "      <td>0.832945</td>\n",
       "      <td>{'tvec__max_df': 0.6, 'tvec__min_df': 3, 'tvec...</td>\n",
       "      <td>0.861468</td>\n",
       "      <td>0.819095</td>\n",
       "      <td>140</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>186</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.707071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(tvec, knn3)</td>\n",
       "      <td>0.766598</td>\n",
       "      <td>{'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...</td>\n",
       "      <td>0.768268</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>137</td>\n",
       "      <td>61</td>\n",
       "      <td>43</td>\n",
       "      <td>157</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.691919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(tvec, knn5)</td>\n",
       "      <td>0.780855</td>\n",
       "      <td>{'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...</td>\n",
       "      <td>0.780855</td>\n",
       "      <td>0.746231</td>\n",
       "      <td>148</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>149</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.747475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(tvec, lr)</td>\n",
       "      <td>0.910197</td>\n",
       "      <td>{'tvec__max_df': 0.8, 'tvec__min_df': 3, 'tvec...</td>\n",
       "      <td>0.93118</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>177</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>189</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.893939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              steps crossval_score  \\\n",
       "0  (cvec, multi_nb)       0.858943   \n",
       "1      (cvec, knn3)       0.506297   \n",
       "2      (cvec, knn5)       0.501262   \n",
       "3        (cvec, lr)       0.979853   \n",
       "4  (tvec, multi_nb)       0.832945   \n",
       "5      (tvec, knn3)       0.766598   \n",
       "6      (tvec, knn5)       0.780855   \n",
       "7        (tvec, lr)       0.910197   \n",
       "\n",
       "                                          opt_params train_score val_score  \\\n",
       "0  {'cvec__max_df': 0.6, 'cvec__max_features': 15...    0.890028  0.854271   \n",
       "1  {'cvec__max_df': 0.6, 'cvec__max_features': 50...    0.874062  0.889447   \n",
       "2  {'cvec__max_df': 0.6, 'cvec__max_features': 50...    0.879094  0.899497   \n",
       "3  {'cvec__max_df': 0.6, 'cvec__max_features': 50...    0.994128  0.994975   \n",
       "4  {'tvec__max_df': 0.6, 'tvec__min_df': 3, 'tvec...    0.861468  0.819095   \n",
       "5  {'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...    0.768268  0.738693   \n",
       "6  {'tvec__max_df': 0.8, 'tvec__min_df': 1, 'tvec...    0.780855  0.746231   \n",
       "7  {'tvec__max_df': 0.8, 'tvec__min_df': 3, 'tvec...     0.93118  0.919598   \n",
       "\n",
       "    tn  fp  fn   tp sensitivity specificity  \n",
       "0  146  52   6  194        0.97    0.737374  \n",
       "1  183  15  29  171       0.855    0.924242  \n",
       "2  186  12  28  172        0.86    0.939394  \n",
       "3  197   1   1  199       0.995    0.994949  \n",
       "4  140  58  14  186        0.93    0.707071  \n",
       "5  137  61  43  157       0.785    0.691919  \n",
       "6  148  50  51  149       0.745    0.747475  \n",
       "7  177  21  11  189       0.945    0.893939  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the results gathered, the pipeline with `CountVectorizer` and `LogisticRegression` returned the highest cross validation score.\n",
    "- It also achieved the highest accuracy score on the training and validation data respectively, with a small difference in between the two scores.\n",
    "- This indicates that the model achieves low bias and low variance, which is optimal.\n",
    "- Sensitivity scores for this pipe is 1, indicating that all the /r/tifu subreddits were predicted correctly\n",
    "- Specificity scores for the pipe is close to 1, indicating a small margin of error for the /r/confessions subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these results, the pipeline selected if pipeline 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('cvec', CountVectorizer(stop_words='english')),\n",
      "                ('lr', LogisticRegression(max_iter=2000, n_jobs=-1))])\n"
     ]
    }
   ],
   "source": [
    "print(pipelines[3]) # Indicating steps in pipeline for selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal `CountVectorizer` parameters are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cvec__max_df': 0.6, 'cvec__max_features': 50, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "print(eval_df['opt_params'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
